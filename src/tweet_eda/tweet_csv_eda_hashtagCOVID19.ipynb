{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to explore our scraped tweets with query of the hashtag \"#COVID19\". In this we are going to first preprocess the tweets, perform some visualisations and later on investigate if there is a correlation between COVID19 and scarcity and if so, try to quantify it. An issue with this very model is that we require a benchmark to compare against, otherwise the values we report are effectively useless. Maybe a better model would be to query the hashtag \"#Hunger\" or \"#Poverty\" and see how many associations there are with coronavirus. Again, past data would come in handly. Perhaps a Premium Twitter Developer account may help and we can form a partnership of some kind with the right people to aid in this quest for analysis of tweets. More visualiastion here than NLP.\n",
    "\n",
    "\n",
    "Author: Steven Vuong<br>\n",
    "Last Edited: 11-05-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries we will be using\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_savepath = \"../../data/tweets.csv\"\n",
    "tweets_df = pd.read_csv(csv_savepath) # Of query \"#COVID19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_creation_date</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_retweet_count</th>\n",
       "      <th>tweet_favourite_count</th>\n",
       "      <th>tweet_hashtags</th>\n",
       "      <th>user_follow_count</th>\n",
       "      <th>user_created_at</th>\n",
       "      <th>user_verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-11 06:52:34</td>\n",
       "      <td>RT @Parvez_Iftikhar: I tried to tell Sama TV t...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1180</td>\n",
       "      <td>2015-07-10 10:57:30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-11 06:52:33</td>\n",
       "      <td>All crowd and no social distancing at Paris ga...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>83205</td>\n",
       "      <td>2013-01-04 14:19:42</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-11 06:52:33</td>\n",
       "      <td>RT @HackneyAbbott: Muddled messaging from @Bor...</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>['coronavirus']</td>\n",
       "      <td>90</td>\n",
       "      <td>2015-09-27 08:00:46</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-11 06:52:33</td>\n",
       "      <td>RT @OpenOrphan: We are very pleased to announc...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>266</td>\n",
       "      <td>2016-07-08 16:01:44</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-11 06:52:33</td>\n",
       "      <td>For @NicolaSturgeon Alert, adjective meaning \\...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>273</td>\n",
       "      <td>2010-11-03 19:07:27</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_creation_date                                         tweet_text  \\\n",
       "0  2020-05-11 06:52:34  RT @Parvez_Iftikhar: I tried to tell Sama TV t...   \n",
       "1  2020-05-11 06:52:33  All crowd and no social distancing at Paris ga...   \n",
       "2  2020-05-11 06:52:33  RT @HackneyAbbott: Muddled messaging from @Bor...   \n",
       "3  2020-05-11 06:52:33  RT @OpenOrphan: We are very pleased to announc...   \n",
       "4  2020-05-11 06:52:33  For @NicolaSturgeon Alert, adjective meaning \\...   \n",
       "\n",
       "   tweet_retweet_count  tweet_favourite_count   tweet_hashtags  \\\n",
       "0                    5                      0               []   \n",
       "1                    0                      0               []   \n",
       "2                  289                      0  ['coronavirus']   \n",
       "3                    1                      0               []   \n",
       "4                    0                      0               []   \n",
       "\n",
       "   user_follow_count      user_created_at  user_verified  \n",
       "0               1180  2015-07-10 10:57:30          False  \n",
       "1              83205  2013-01-04 14:19:42           True  \n",
       "2                 90  2015-09-27 08:00:46          False  \n",
       "3                266  2016-07-08 16:01:44          False  \n",
       "4                273  2010-11-03 19:07:27          False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try get some ideas for preprocessing from other notebooks/kernels.\n",
    "\n",
    "-  Idea: Try to predict if tweet contains hashtag \"#Hunger\" or related. Can use tweet text and other data to try and predict then ensemble. Brilliant template for model training: https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n",
    "-  Preprocessing: https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing\n",
    "-  Visualisation: https://www.kaggle.com/duttadebadri/detailed-nlp-project-prediction-visualization\n",
    "    - Histogram/barplots for frequency\n",
    "    - Wordcloud (Hoping to do post processing)\n",
    "    - Word correlation map\n",
    "    \n",
    "Also to consider: unigrams/bigrams/trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/steven/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/steven/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/steven/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords, \\\n",
    "    wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# from collections import Counter # Used later for frequent and rare words\n",
    "import re\n",
    "from language_dicts.emoticons_dict import EMOTICONS\n",
    "from language_dicts.emojis_dict import UNICODE_EMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Class containing typical functions for text preprocessing.\n",
    "    Can swap ordering of preprocessing functions if so desired.\n",
    "    Args:\n",
    "        - text(str)\n",
    "    Note:\n",
    "        - Appears lemmatize is more effective than stemming\n",
    "    Ref:\n",
    "        - https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing\n",
    "    \"\"\"\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text\n",
    "        \n",
    "    def _make_lowercase(self):\n",
    "        self.text = self.text.lower()\n",
    "        \n",
    "    def _remove_punctuation(self):\n",
    "        \"\"\"Note: We do not include '@' or because it is used to reference \n",
    "        other twitter accounts\"\"\"\n",
    "        punctuation_to_remove=\"\\\"\\!#$%&\\'()*+,-./:;<=>?[\\\\]^_{|}~`\"\n",
    "        self.text = self.text.translate(\n",
    "            str.maketrans('', '', punctuation_to_remove)\n",
    "        )\n",
    "    \n",
    "    def _remove_stopwords(self):\n",
    "        nltk_stopwords = set(stopwords.words('english'))\n",
    "        self.text = \" \".join([\n",
    "            word for word in str(self.text).split() if word not in nltk_stopwords\n",
    "        ])\n",
    "        \n",
    "    def _stem(self):\n",
    "        \"\"\"Note: Can also use SnowballStemmer for languages other than English\"\"\"\n",
    "        stemmer = PorterStemmer()\n",
    "        self.text = \" \".join([stemmer.stem(word) for word in self.text.split()])\n",
    "        \n",
    "    def _lemmatize(self):\n",
    "        \"\"\"Note: Different Lemmatizing options: N (noun), \n",
    "        V (verb), J (adjective) and R (adverb)\n",
    "        \"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        wordnet_map = {\n",
    "            \"N\":wordnet.NOUN, \n",
    "            \"V\":wordnet.VERB, \n",
    "            \"J\":wordnet.ADJ, \n",
    "            \"R\":wordnet.ADV\n",
    "        }\n",
    "        pos_tagged_text = nltk.pos_tag(self.text.split())\n",
    "        self.text = \" \".join([\n",
    "            lemmatizer.lemmatize(\n",
    "                word, wordnet_map.get(pos[0], wordnet.NOUN)\n",
    "            ) for word, pos in pos_tagged_text\n",
    "        ])\n",
    "        \n",
    "    def _remove_emojis(self):\n",
    "        \"\"\"Ref: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\"\"\"\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       \"]+\", flags=re.UNICODE\n",
    "                                  )\n",
    "        self.text = emoji_pattern.sub(r'', self.text)\n",
    "        \n",
    "    def _convert_emojis_to_words(self):\n",
    "        for emoji in UNICODE_EMO:\n",
    "            self.text = re.sub(\n",
    "                r'('+emoji+')', \n",
    "                \"_\".join(UNICODE_EMO[emoji].replace(\",\",\"\").replace(\":\",\"\").split()), \n",
    "                self.text\n",
    "            )\n",
    "        \n",
    "    def _remove_emoticons(self):\n",
    "        emoticon_pattern = re.compile(\n",
    "            u'(' + u'|'.join(k for k in EMOTICONS) + u')'\n",
    "        )\n",
    "        self.text = emoticon_pattern.sub(r'', self.text)\n",
    "        \n",
    "    def _convert_emoticons_to_words(self):\n",
    "        for emoticon in EMOTICONS:\n",
    "            self.text = re.sub(\n",
    "                u'('+emoticon+')', \n",
    "                \"_\".join(EMOTICONS[emoticon].replace(\",\",\"\").split()), \n",
    "                self.text\n",
    "            )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Sample Text-----------\n",
      "RT @Parvez_Iftikhar: I tried to tell Sama TV that our 4G is badly in need of improvement. Govt needs to give some spectrum plus some tax re…\n",
      "\n",
      "-----------Make Lowercase-----------\n",
      "rt @parvez_iftikhar: i tried to tell sama tv that our 4g is badly in need of improvement. govt needs to give some spectrum plus some tax re…\n",
      "\n",
      "-----------Remove Punctuation-----------\n",
      "rt @parveziftikhar i tried to tell sama tv that our 4g is badly in need of improvement govt needs to give some spectrum plus some tax re…\n",
      "\n",
      "-----------Remove Stopwords-----------\n",
      "rt @parveziftikhar tried tell sama tv 4g badly need improvement govt needs give spectrum plus tax re…\n",
      "\n",
      "-----------Lemmatize-----------\n",
      "rt @parveziftikhar try tell sama tv 4g badly need improvement govt need give spectrum plus tax re…\n",
      "\n",
      "-----------Remove Emoji-----------\n",
      "rt @parveziftikhar try tell sama tv 4g badly need improvement govt need give spectrum plus tax re…\n",
      "\n",
      "-----------Remove Emoticons-----------\n",
      "rt @parveziftikhar try tell sama tv 4g badly need improvement govt need give spectrum plus tax re…\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------Sample Text-----------\")\n",
    "txt_sample = TextPreprocessor(tweets_df[\"tweet_text\"][0])\n",
    "print(txt_sample.text)\n",
    "\n",
    "# Make lowercase\n",
    "txt_sample._make_lowercase()\n",
    "\n",
    "# Remove punctuation\n",
    "txt_sample._remove_punctuation()\n",
    "print(\"\\n-----------Remove Punctuation-----------\")\n",
    "print(txt_sample.text)\n",
    "\n",
    "# Remove stopwords\n",
    "txt_sample._remove_stopwords()\n",
    "print(\"\\n-----------Remove Stopwords-----------\")\n",
    "print(txt_sample.text)\n",
    "\n",
    "# Lemmatize words\n",
    "txt_sample._lemmatize()\n",
    "print(\"\\n-----------Lemmatize-----------\")\n",
    "print(txt_sample.text)\n",
    "\n",
    "# Remove emojis\n",
    "txt_sample._remove_emojis()\n",
    "print(\"\\n-----------Remove Emoji-----------\")\n",
    "print(txt_sample.text)\n",
    "\n",
    "# Remove emoticons\n",
    "txt_sample._remove_emoticons()\n",
    "print(\"\\n-----------Remove Emoticons-----------\")\n",
    "print(txt_sample.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes (for this use case):\n",
    "\n",
    "- We opt for lemmatizing in this use case, just that the example output looks better >> Would.\n",
    "\n",
    "We will opt for removing emoticons and emojis than \n",
    "converting to words. Just want to have a very simple\n",
    "model to begin with and conversion may not be\n",
    "so greatly depicted in tweets. May be for other use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Make a plot function -> histogram, worcloud ettc..\n",
    "    # Or does that belong in a separate func? Maybe clas with static\n",
    "    # metthods hauah!\n",
    "# Remove frequent words would have to be performed outside as analysis of \n",
    "# all the words first, then remove. Same applies for removing rare words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Also considerr stanford NER (named entity recognition) Tagger\n",
    "# https://towardsdatascience.com/tweet-analytics-using-nlp-f83b9f7f7349\n",
    "# from nltk.tag import StanfordNERTagger\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# st = StanfordNERTagger('/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz','/usr/share/stanford-ner/stanford-ner.jar',encoding='utf-8')\n",
    "# #a tweet by Donald Trump\n",
    "# text = 'Just had a very good call with @SwedishPM Stefan Löfven who assured me that American citizen A$AP Rocky will be treated fairly. Likewise, I assured him that A$AP was not a flight risk and offered to personally vouch for his bail, or an alternative....'\n",
    "\n",
    "# tokenized_text = word_tokenize(text)\n",
    "# classified_text = st.tag(tokenized_text)\n",
    "\n",
    "# print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
