{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to explore our scraped tweets with query of the hashtag \"#COVID19\". In this we are going to first preprocess the tweets, perform some visualisations and later on investigate if there is a correlation between COVID19 and scarcity and if so, try to quantify it. An issue with this very model is that we require a benchmark to compare against, otherwise the values we report are effectively useless. Maybe a better model would be to query the hashtag \"#Hunger\" or \"#Poverty\" and see how many associations there are with coronavirus. Again, past data would come in handly. Perhaps a Premium Twitter Developer account may help and we can form a partnership of some kind with the right people to aid in this quest for analysis of tweets. More visualiastion here than NLP.\n",
    "\n",
    "\n",
    "Author: Steven Vuong<br>\n",
    "Last Edited: 11-05-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_savepath = \"../../data/tweets.csv\"\n",
    "tweets_df = pd.read_csv(csv_savepath) # Of query \"#COVID19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try get some ideas for preprocessing from other notebooks/kernels.\n",
    "\n",
    "-  Idea: Try to predict if tweet contains hashtag \"#Hunger\" or related. Can use tweet text and other data to try and predict then ensemble. Brilliant template for model training: https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n",
    "-  Preprocessing: https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing\n",
    "-  Visualisation: https://www.kaggle.com/duttadebadri/detailed-nlp-project-prediction-visualization\n",
    "    - Histogram/barplots for frequency\n",
    "    - Wordcloud (Hoping to do post processing)\n",
    "    - Word correlation map\n",
    "    \n",
    "Also to consider: unigrams/bigrams/trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocessor import TextPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"-----------Sample Text-----------\")\n",
    "txt_sample = TextPreprocessor(tweets_df[\"tweet_text\"][0])\n",
    "print(txt_sample.text)\n",
    "\n",
    "# Apply preprocessing steps\n",
    "txt_sample._make_lowercase()\n",
    "txt_sample._remove_punctuation()\n",
    "txt_sample._remove_stopwords()\n",
    "txt_sample._lemmatize()\n",
    "txt_sample._remove_emojis()\n",
    "txt_sample._remove_emoticons()\n",
    "txt_sample._remove_urls()\n",
    "txt_sample._remove_html_tags()\n",
    "txt_sample._spellcheck()\n",
    "\n",
    "print(\"\\n----------Post Processed-----------\")\n",
    "print(txt_sample.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing notes for this use case:\n",
    "\n",
    "- We opt for lemmatizing in this use case, just that the example output looks better >> Would require more samples to be sure.\n",
    "- We opt for removing emoticons and emojis than converting to words as we want to have a simple model to begin with and conversion may not be so greatly depicted in tweets. May be for other use cases\n",
    "- Keeping slang as it may contain important nuances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Add Stanford NER tagger to TextProcessor Class\n",
    "# Also considerr stanford NER (named entity recognition) Tagger\n",
    "# https://towardsdatascience.com/tweet-analytics-using-nlp-f83b9f7f7349\n",
    "# from nltk.tag import StanfordNERTagger\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# st = StanfordNERTagger('/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz','/usr/share/stanford-ner/stanford-ner.jar',encoding='utf-8')\n",
    "# #a tweet by Donald Trump\n",
    "# text = 'Just had a very good call with @SwedishPM Stefan Löfven who assured me that American citizen A$AP Rocky will be treated fairly. Likewise, I assured him that A$AP was not a flight risk and offered to personally vouch for his bail, or an alternative....'\n",
    "\n",
    "# tokenized_text = word_tokenize(text)\n",
    "# classified_text = st.tag(tokenized_text)\n",
    "\n",
    "# print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Make a plot function -> histogram, worcloud ettc..\n",
    "    # Or does that belong in a separate func? Maybe clas with static\n",
    "    # metthods hauah!\n",
    "# Remove frequent words would have to be performed outside as analysis of \n",
    "# all the words first, then remove. Same applies for removing rare words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove most frequent words and most rare words from text\n",
    "# TODO: Make a loop to process all the words in our dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
